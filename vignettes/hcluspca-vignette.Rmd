---
title: Hierarchical Clustering PCA with hcluspca
output: rmarkdown::html_vignette
vignette: |
  %\VignetteIndexEntry{Hierarchical Clustering PCA with hcluspca}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
params:
  family: red
css: albers.css
resource_files:
- albers.css
- albers.js
includes:
  in_header: |-
    <script src="albers.js"></script>
    <script>document.addEventListener('DOMContentLoaded',function(){document.body.classList.add('palette-red');});</script>

---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  message = FALSE,
  warning = FALSE
)
```

```{r setup}
if (requireNamespace("ggplot2", quietly = TRUE) && requireNamespace("albersdown", quietly = TRUE)) ggplot2::theme_set(albersdown::theme_albers(params$family))
library(subpca)
library(multivarious)
library(dendextend)
```

## Overview

The `hcluspca()` function builds a multilevel PCA model from a hierarchical clustering.

Current API expectations:

1. `hclus` is a precomputed `hclust` object.
2. `cuts` is an increasing integer vector (coarse to fine levels).
3. `ccomp` controls retained components per fitted level (global + each cut level).

## Simulated Data

```{r simulated-data}
set.seed(789)

n_obs <- 100
n_vars <- 60

X <- matrix(rnorm(n_obs * n_vars), n_obs, n_vars)

# Inject row-wise structure so hierarchical clustering over observations is meaningful.
X[1:50, 1:20] <- X[1:50, 1:20] + 1.5
X[51:100, 21:40] <- X[51:100, 21:40] - 1.2

# Build observation hierarchy.
hclus_obs <- hclust(dist(X), method = "ward.D2")
plot(hclus_obs, main = "Observation Hierarchy", xlab = "Observations", ylab = "Height")
```

## Basic hcluspca Fit

```{r basic-fit}
# ccomp has one entry for global level + one for each element of cuts.
fit <- hcluspca(
  X,
  hclus = hclus_obs,
  cuts = c(2, 4),
  ccomp = c(3, 2, 1),
  orthogonalize = FALSE
)

cat("Model levels:", fit$levels + 1, "(global +", fit$levels, "cut levels)\n")
cat("Cuts:", paste(fit$cuts, collapse = ", "), "\n")
cat("Total retained components:", ncomp(fit), "\n")
cat("Scores shape:", paste(dim(scores(fit)), collapse = " x "), "\n")
cat("Loadings shape:", paste(dim(components(fit)), collapse = " x "), "\n")
```

## Choosing Different Hierarchies

```{r linkage-methods}
# Compare linkage strategies by changing only the hclust object.
hc_complete <- hclust(dist(X), method = "complete")
hc_average <- hclust(dist(X), method = "average")
hc_ward <- hclust(dist(X), method = "ward.D2")

fit_complete <- hcluspca(X, hclus = hc_complete, cuts = c(2), ccomp = c(3, 2))
fit_average <- hcluspca(X, hclus = hc_average, cuts = c(2), ccomp = c(3, 2))
fit_ward <- hcluspca(X, hclus = hc_ward, cuts = c(2), ccomp = c(3, 2))

cat("Total components by linkage:\n")
cat("  complete:", ncomp(fit_complete), "\n")
cat("  average:", ncomp(fit_average), "\n")
cat("  ward.D2:", ncomp(fit_ward), "\n")
```

## Function-Based Component Selection

```{r function-ccomp}
# ccomp can be a function; this function adapts retained components by level fit rank.
ccomp_fun <- function(fit, i) {
  max(1, min(3, floor(sqrt(ncomp(fit)))))
}

fit_fun <- hcluspca(
  X,
  hclus = hclus_obs,
  cuts = c(2, 4),
  ccomp = ccomp_fun,
  orthogonalize = FALSE
)

cat("Total components with functional ccomp:", ncomp(fit_fun), "\n")
cat("Score matrix shape:", paste(dim(scores(fit_fun)), collapse = " x "), "\n")
```

## Projecting New Data

```{r projection}
X_new <- X[1:10, ] + matrix(rnorm(10 * n_vars, sd = 0.1), 10, n_vars)
new_scores <- project(fit, X_new)

cat("Projected score shape:", paste(dim(new_scores), collapse = " x "), "\n")
```

## Notes

- `hcluspca()` currently clusters observations internally (`colwise = FALSE` in nested `clusterpca()` calls).
- `cuts` must be strictly increasing and greater than 1.
- For stable examples, use `orthogonalize = FALSE` unless you are explicitly testing orthogonalization behavior.
